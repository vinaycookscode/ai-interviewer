// System Design - 25 Detailed Flashcards
export const SYSTEM_DESIGN_CARDS = [
    { front: "What is the difference between horizontal and vertical scaling?", back: "**Vertical Scaling (Scale Up):**\n- Add more power to existing machine (CPU, RAM, storage)\n- Simpler, no code changes\n- Has hardware limits\n- Single point of failure\n\n**Horizontal Scaling (Scale Out):**\n- Add more machines to the pool\n- Better for handling traffic spikes\n- Requires load balancer\n- More complex (distributed systems)\n- Provides redundancy\n\n**Rule:** Start vertical, go horizontal when needed." },
    { front: "Explain the CAP Theorem.", back: "**CAP Theorem:** A distributed system can only guarantee 2 of 3:\n\n**C - Consistency:** All nodes see same data at same time\n**A - Availability:** Every request gets a response\n**P - Partition Tolerance:** System works despite network failures\n\n**In practice, P is required, so choose:**\n- **CP Systems:** Sacrifice availability (e.g., MongoDB, Redis)\n- **AP Systems:** Sacrifice consistency (e.g., Cassandra, DynamoDB)\n\n**Most systems:** Eventual consistency (AP with tunable consistency)" },
    { front: "What are common load balancing algorithms?", back: "**1. Round Robin:** Rotate through servers sequentially\n- Simple, equal distribution\n- Ignores server capacity\n\n**2. Weighted Round Robin:** Higher weight = more requests\n- For servers with different capacities\n\n**3. Least Connections:** Send to server with fewest active connections\n- Better for varying request durations\n\n**4. IP Hash:** Hash client IP to determine server\n- Session persistence (same client → same server)\n\n**5. Random:** Randomly select server\n- Simple, works well at scale\n\n**Layer 4 vs Layer 7:**\n- L4: TCP/UDP level (faster)\n- L7: HTTP level (can route by URL, headers)" },
    { front: "When and how would you use database sharding?", back: "**Sharding:** Splitting data across multiple databases\n\n**When to shard:**\n- Single DB can't handle load\n- Data too large for one machine\n\n**Strategies:**\n1. **Hash-based:** hash(key) % num_shards\n   - Even distribution\n   - Hard to add shards\n\n2. **Range-based:** User IDs 1-1M → shard 1, 1M-2M → shard 2\n   - Easy range queries\n   - Can have hot spots\n\n3. **Directory-based:** Lookup table maps key → shard\n   - Flexible\n   - Lookup adds latency\n\n**Challenges:**\n- Cross-shard queries are expensive\n- Rebalancing is complex\n- Joins become harder" },
    { front: "Explain ACID properties in databases.", back: "**A - Atomicity:**\n- Transaction is all-or-nothing\n- Either all operations succeed, or none do\n\n**C - Consistency:**\n- Database moves from one valid state to another\n- All constraints are satisfied\n\n**I - Isolation:**\n- Concurrent transactions don't interfere\n- Each sees consistent snapshot\n\n**D - Durability:**\n- Once committed, data survives crashes\n- Usually via write-ahead logging\n\n**vs BASE (NoSQL):**\n- **B**asically **A**vailable\n- **S**oft state\n- **E**ventually consistent\n\nACID = strong consistency, BASE = high availability" },
    { front: "What is consistent hashing and why is it used?", back: "**Consistent Hashing:** Distribute data across nodes with minimal redistribution when nodes change.\n\n**How it works:**\n1. Hash both keys and nodes onto a ring (0 to 2^32)\n2. Key is assigned to first node clockwise from its position\n3. When node added/removed, only nearby keys move\n\n**Benefits:**\n- Only K/N keys remapped when adding/removing nodes\n- vs Regular hashing: all keys remapped\n\n**Virtual nodes:**\n- Each physical node has multiple positions on ring\n- Better load distribution\n- Handle heterogeneous servers\n\n**Used in:** Memcached, Cassandra, DynamoDB, CDNs" },
    { front: "Explain different rate limiting algorithms.", back: "**1. Token Bucket:**\n- Bucket holds tokens, refilled at constant rate\n- Request consumes token\n- Allows bursts up to bucket size\n\n**2. Leaky Bucket:**\n- Fixed outflow rate\n- Excess requests queue or drop\n- Smooths traffic\n\n**3. Fixed Window:**\n- Count requests per time window\n- Problem: burst at window boundary\n\n**4. Sliding Window Log:**\n- Track timestamp of each request\n- Count in sliding window\n- Accurate but memory intensive\n\n**5. Sliding Window Counter:**\n- Weighted average of current and previous window\n- Good balance of accuracy and memory" },
    { front: "What are the different caching strategies?", back: "**1. Cache-Aside (Lazy Loading):**\n- App checks cache, if miss reads DB, writes to cache\n- Most common pattern\n- Cache can become stale\n\n**2. Read-Through:**\n- Cache sits between app and DB\n- Cache loads from DB on miss\n- App only talks to cache\n\n**3. Write-Through:**\n- Write to cache and DB together\n- Consistent but slower writes\n\n**4. Write-Behind (Write-Back):**\n- Write to cache, async write to DB\n- Fast writes but risk of data loss\n\n**5. Refresh-Ahead:**\n- Proactively refresh before expiration\n- Good for hot data\n\n**Eviction:** LRU, LFU, FIFO, Random" },
    { front: "How would you design a URL shortener like bit.ly?", back: "**Requirements:**\n- Shorten long URL\n- Redirect short → long\n- High read, lower write\n\n**Short URL Generation:**\n1. **Counter + Base62:** Increment counter, encode to base62\n   - 7 chars = 62^7 = 3.5 trillion URLs\n2. **Hash + Truncate:** MD5/SHA, take first 7 chars, handle collisions\n\n**Architecture:**\n```\nClient → Load Balancer → Web Servers → Cache (Redis)\n                                        ↓\n                                    Database (Key-Value)\n```\n\n**Database:** Key-value store (short_url → long_url, metadata)\n\n**Caching:** Cache popular URLs in Redis\n\n**Analytics:** Async logging to analytics DB\n\n**Scaling:** Shard by short URL hash" },
    { front: "How would you design a Twitter-like feed system?", back: "**Key Components:**\n1. Tweet Service: Create, store tweets\n2. Fan-out Service: Distribute to followers\n3. Timeline Service: Build user's feed\n\n**Fan-out Strategies:**\n\n**Push (Fan-out on write):**\n- Write tweet to all followers' timelines\n- Pros: Fast reads\n- Cons: Slow writes for celebrities, storage cost\n\n**Pull (Fan-out on read):**\n- Query all followed users on read\n- Pros: Fast writes\n- Cons: Slow reads\n\n**Hybrid (Twitter's approach):**\n- Push for normal users\n- Pull for celebrities (> 1M followers)\n- Combine at read time\n\n**Storage:**\n- Tweet data: SQL/NoSQL\n- Timeline: Redis sorted set (tweet_id, timestamp)" },
    { front: "What is event sourcing and when would you use it?", back: "**Event Sourcing:** Store sequence of events instead of current state.\n\n**Traditional:** Update user.balance = 100\n**Event Sourcing:** Store events: Deposited(50), Withdrew(30), Deposited(80)\n\n**Benefits:**\n- Complete audit trail\n- Can rebuild state at any point in time\n- Easy temporal queries (\"what was state on date X?\")\n- Natural for event-driven systems\n\n**Challenges:**\n- More complex queries (need to replay events)\n- Event schema evolution\n- Storage grows over time (use snapshots)\n\n**Use cases:**\n- Financial systems\n- Audit requirements\n- Debugging and replay\n- CQRS pattern" },
    { front: "Explain the CQRS pattern.", back: "**CQRS:** Command Query Responsibility Segregation\n\n**Idea:** Separate read and write models\n\n**Write side (Commands):**\n- Create, update, delete operations\n- Optimized for writes\n- Can use normalized data\n\n**Read side (Queries):**\n- Read-only operations\n- Optimized for specific queries\n- Can use denormalized views\n\n**Architecture:**\n```\nWrite Commands → Write Model → Event Store\n                                    ↓ (Projections)\n Read Queries ← Read Model ← Materialized Views\n```\n\n**Benefits:**\n- Independent scaling of reads vs writes\n- Optimized data models for each\n- Works well with event sourcing\n\n**Trade-offs:**\n- Eventual consistency between models\n- More complex" },
    { front: "How would you design a distributed message queue like Kafka?", back: "**Core Concepts:**\n- **Topics:** Categories of messages\n- **Partitions:** Ordered, immutable sequence (parallelism unit)\n- **Producers:** Write messages to topics\n- **Consumers:** Read from topics via consumer groups\n\n**Architecture:**\n```\nProducers → Brokers (partitions) → Consumers\n                ↓\n            ZooKeeper (coordination)\n```\n\n**Key Features:**\n- Messages persist on disk (log-based)\n- Consumers track offset (position)\n- Replication for fault tolerance\n- Consumer groups for parallel processing\n\n**Partition assignment:**\n- Each partition → one consumer in group\n- Order guaranteed within partition\n\n**Retention:** Time-based or size-based" },
    { front: "How would you design a rate limiter?", back: "**Functional Requirements:**\n- Limit requests per user/IP\n- Return 429 when exceeded\n- Distributed across servers\n\n**Algorithm:** Token Bucket or Sliding Window\n\n**Storage:** Redis for distributed counter\n```\nkey: rate_limiter:{user_id}\nvalue: {count, window_start} or token count\n```\n\n**Implementation:**\n```python\ndef is_allowed(user_id, limit, window):\n    key = f\"rate_limiter:{user_id}\"\n    current = redis.get(key)\n    \n    if current and int(current) >= limit:\n        return False\n    \n    pipe = redis.pipeline()\n    pipe.incr(key)\n    pipe.expire(key, window)  # Reset after window\n    pipe.execute()\n    return True\n```\n\n**Edge cases:**\n- Race conditions (use Lua scripts)\n- Distributed sync (Redis cluster)\n- Graceful degradation" },
    { front: "What is a CDN and how does it work?", back: "**CDN (Content Delivery Network):** Geographically distributed servers caching content close to users.\n\n**How it works:**\n1. User requests content\n2. DNS routes to nearest CDN edge\n3. Edge serves from cache (hit) or fetches from origin (miss)\n4. Cache content with TTL\n\n**Benefits:**\n- Reduced latency (content closer to users)\n- Reduced origin load\n- Handle traffic spikes\n- DDoS protection\n\n**Cache Control:**\n- TTL (Time To Live)\n- Cache-Control headers\n- Purge/Invalidation APIs\n\n**Types of content:**\n- Static: Images, CSS, JS, videos\n- Dynamic: API responses (shorter TTL)\n\n**Providers:** CloudFlare, AWS CloudFront, Akamai" },
    { front: "How would you design a chat application like WhatsApp?", back: "**Key Components:**\n\n**1. Connection:** WebSocket for real-time\n- Keep persistent connection\n- Heartbeat for connection health\n\n**2. Message Flow:**\n```\nSender → Chat Server → Message Queue → Chat Server → Receiver\n                            ↓\n                        Database\n```\n\n**3. Features:**\n- Online status: Presence service\n- Read receipts: Store delivery/read status\n- Group chat: Fan-out to members\n- Media: Store in blob storage, share URL\n\n**4. Offline handling:**\n- Store messages in queue/DB\n- Push notification\n- Sync on reconnect\n\n**Storage:**\n- Messages: NoSQL (Cassandra) - partitioned by chat_id\n- User data: SQL\n- Media: S3/Blob storage" },
    { front: "What are microservices and when should you use them?", back: "**Microservices:** Architecture where application is split into small, independent services.\n\n**Characteristics:**\n- Single responsibility\n- Independent deployment\n- Own database\n- Communicate via APIs\n\n**When to use:**\n✅ Large teams (Conway's Law)\n✅ Need independent scaling\n✅ Different tech stacks per service\n✅ Frequent, independent deployments\n\n**When NOT to use:**\n❌ Small team (< 10 engineers)\n❌ Simple application\n❌ Unclear domain boundaries\n\n**Challenges:**\n- Distributed system complexity\n- Network latency\n- Data consistency\n- Debugging/tracing\n\n**Supporting infrastructure:**\n- Service discovery\n- API Gateway\n- Circuit breakers\n- Distributed tracing" },
    { front: "Explain the Circuit Breaker pattern.", back: "**Circuit Breaker:** Prevent cascading failures by failing fast when a service is unhealthy.\n\n**States:**\n1. **Closed:** Normal operation, requests pass through\n   - Track failure rate\n   - If threshold exceeded → Open\n\n2. **Open:** Fail immediately without calling service\n   - After timeout → Half-Open\n\n3. **Half-Open:** Allow limited requests to test recovery\n   - If succeed → Closed\n   - If fail → Open\n\n**Implementation:**\n```python\nclass CircuitBreaker:\n    def call(self, func):\n        if self.state == OPEN:\n            if time.now() > self.reset_time:\n                self.state = HALF_OPEN\n            else:\n                raise CircuitOpenError()\n        \n        try:\n            result = func()\n            self.on_success()\n            return result\n        except Exception:\n            self.on_failure()\n            raise\n```\n\n**Libraries:** Hystrix, resilience4j, Polly" },
    { front: "How would you design a notification system?", back: "**Requirements:**\n- Multiple channels: Push, SMS, Email\n- Priority levels\n- User preferences\n- Reliable delivery\n\n**Architecture:**\n```\nServices → Notification Service → Priority Queue → Workers\n                ↓                                    ↓\n        User Preferences                    Channel Providers\n                                            (APNS, FCM, Twilio, SES)\n```\n\n**Flow:**\n1. Service sends notification request\n2. Check user preferences (opted out?)\n3. Add to priority queue\n4. Workers process and send to providers\n5. Track delivery status\n\n**Key considerations:**\n- Rate limiting per user\n- Retry with backoff\n- Template management\n- Batching for efficiency\n- Analytics and tracking" },
    { front: "What is database replication and what are the types?", back: "**Replication:** Copying data across multiple servers.\n\n**Types:**\n\n**1. Master-Slave (Primary-Replica):**\n- One master handles writes\n- Slaves handle reads\n- Async replication (eventual consistency)\n- Use: Read-heavy workloads\n\n**2. Master-Master (Multi-Master):**\n- Multiple nodes accept writes\n- Conflict resolution needed\n- Use: High write availability\n\n**3. Synchronous vs Async:**\n- Sync: Wait for replica confirmation (strong consistency, slower)\n- Async: Don't wait (faster, possible data loss)\n\n**Replication Lag:**\n- Time between master write and replica update\n- Solutions: Read-your-writes, causal consistency\n\n**Failover:**\n- Automatic: Heartbeat monitoring, auto-promote\n- Manual: Operator intervention" },
    { front: "How do you ensure data consistency in distributed systems?", back: "**Consistency Models:**\n\n**Strong Consistency:**\n- All reads see latest write\n- Implemented via: Consensus (Paxos, Raft)\n- Trade-off: Higher latency\n\n**Eventual Consistency:**\n- Reads may see stale data temporarily\n- Eventually all replicas converge\n- Trade-off: Possible stale reads\n\n**Techniques:**\n\n**1. Distributed Transactions:**\n- 2PC (Two-Phase Commit)\n- Saga pattern (compensating transactions)\n\n**2. Consensus Algorithms:**\n- Raft, Paxos\n- Leader election and log replication\n\n**3. Vector Clocks/Version Vectors:**\n- Track causality\n- Detect conflicts\n\n**4. CRDTs (Conflict-free Replicated Data Types):**\n- Data structures that auto-merge\n- e.g., Counters, Sets" },
    { front: "What is a Bloom Filter and when would you use it?", back: "**Bloom Filter:** Probabilistic data structure for set membership.\n\n**Properties:**\n- Can tell you: \"definitely not in set\" or \"possibly in set\"\n- False positives possible\n- False negatives impossible\n- Very space efficient\n\n**How it works:**\n1. Bit array of m bits\n2. k hash functions\n3. Insert: Hash item k times, set those bits to 1\n4. Lookup: Check if all k bits are 1\n\n**Use cases:**\n- Avoid expensive lookups (check Bloom first)\n- Spell checkers\n- Cache: \"Has this key been seen?\"\n- Database: \"Is this row in table?\"\n- Malware detection: \"Is this URL malicious?\"\n\n**Example:** Before querying database, check Bloom filter. If \"no\", skip the query." },
    { front: "How would you design a search autocomplete system?", back: "**Requirements:**\n- Fast suggestions as user types\n- Personalized + popular results\n- Real-time updates\n\n**Data Structure:** Trie (Prefix Tree)\n```\n       root\n      / | \\\n     a  b  c\n    /|     |\n   p m     a\n   |       |\n  ple     r\n```\n\n**Architecture:**\n```\nUser types → API Gateway → Autocomplete Service → Trie (in-memory)\n                                    ↓\n                            Aggregation Service\n                                    ↓\n                            Analytics (Kafka → Spark)\n```\n\n**Ranking:**\n- Frequency/popularity\n- Recency\n- User's search history\n- Trending\n\n**Optimization:**\n- Cache top suggestions per prefix\n- Shard tries by first character\n- Update trie offline, swap atomically" },
    { front: "What are the key metrics for system design interviews?", back: "**Latency:**\n- p50, p95, p99 response times\n- Target: < 100ms for real-time, < 1s for web\n\n**Throughput:**\n- Requests per second (RPS)\n- QPS (Queries per second)\n\n**Availability:**\n- % uptime (99.9% = 8.76 hrs downtime/year)\n- Measured by SLA/SLO\n\n**Storage Estimates:**\n- Daily active users × data per user\n- Consider growth over 5 years\n\n**Bandwidth:**\n- Peak traffic × request size\n\n**Back-of-envelope calculations:**\n- 1 day = 86,400 sec ≈ 100K seconds\n- 1 month = 2.5M seconds\n- 1 year = 30M seconds\n- 1M requests/day ≈ 12 RPS\n- 1B requests/day ≈ 12K RPS" },
    { front: "How do you approach a system design interview?", back: "**Step 1: Requirements (5 min)**\n- Functional: What features?\n- Non-functional: Scale, latency, availability\n- Ask clarifying questions!\n\n**Step 2: Estimates (5 min)**\n- Users, QPS, storage, bandwidth\n- Back-of-envelope math\n\n**Step 3: High-Level Design (10 min)**\n- Draw main components\n- Data flow\n- API design\n\n**Step 4: Deep Dive (15 min)**\n- Database schema\n- Caching strategy\n- Scaling approach\n- Handle edge cases\n\n**Step 5: Wrap Up (5 min)**\n- Trade-offs made\n- Future improvements\n- Monitoring/observability\n\n**Tips:**\n- Drive the conversation\n- Think out loud\n- Consider trade-offs\n- Start simple, add complexity" },
];
